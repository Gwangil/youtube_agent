"""
ì„ë² ë”© ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
í•œêµ­ì–´ YouTube ì½˜í…ì¸ ì— ìµœì í™”ëœ ëª¨ë¸ ì„ íƒì„ ìœ„í•œ ì„±ëŠ¥ ì¸¡ì •
"""

import time
import torch
import numpy as np
from typing import List, Dict, Any
import json
from sentence_transformers import SentenceTransformer
import os

# í…ŒìŠ¤íŠ¸ìš© í•œêµ­ì–´ ë¬¸ì¥ë“¤ (YouTube ì½˜í…ì¸  ì‹œë®¬ë ˆì´ì…˜)
TEST_SENTENCES_KO = [
    "ì˜¤ëŠ˜ì€ ì£¼ì‹ì‹œì¥ì´ í° í­ìœ¼ë¡œ ìƒìŠ¹í–ˆìŠµë‹ˆë‹¤. ì½”ìŠ¤í”¼ ì§€ìˆ˜ëŠ” 3% ìƒìŠ¹í•˜ë©° 3,000í¬ì¸íŠ¸ë¥¼ ëŒíŒŒí–ˆìŠµë‹ˆë‹¤.",
    "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì˜ ë°œì „ìœ¼ë¡œ ë§ì€ ì‚°ì—…ì´ ë³€í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ìë™í™”ì™€ íš¨ìœ¨ì„±ì´ í¬ê²Œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.",
    "ìµœê·¼ ë¶€ë™ì‚° ì‹œì¥ì´ ì•ˆì •ì„¸ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. ì •ë¶€ì˜ ê·œì œ ì •ì±…ì´ íš¨ê³¼ë¥¼ ë³´ëŠ” ê²ƒìœ¼ë¡œ ë¶„ì„ë©ë‹ˆë‹¤.",
    "í•œêµ­ ê²½ì œëŠ” ë°˜ë„ì²´ ìˆ˜ì¶œì— í¬ê²Œ ì˜ì¡´í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê¸€ë¡œë²Œ ìˆ˜ìš” ì¦ê°€ë¡œ ìˆ˜ì¶œì´ í˜¸ì¡°ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.",
    "ë¯¸êµ­ ì—°ì¤€ì˜ ê¸ˆë¦¬ ì¸ìƒì´ ê¸€ë¡œë²Œ ê²½ì œì— ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆìŠµë‹ˆë‹¤. í•œêµ­ ì‹œì¥ë„ ë³€ë™ì„±ì´ ì»¤ì§€ê³  ìˆìŠµë‹ˆë‹¤.",
    "ì „ê¸°ì°¨ ì‹œì¥ì´ ë¹ ë¥´ê²Œ ì„±ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤. í…ŒìŠ¬ë¼ì™€ í˜„ëŒ€ì°¨ê°€ ì‹œì¥ì„ ì„ ë„í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
    "ì•”í˜¸í™”í ì‹œì¥ì´ ë‹¤ì‹œ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤. ë¹„íŠ¸ì½”ì¸ ê°€ê²©ì´ ìƒìŠ¹ì„¸ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.",
    "K-íŒì´ ì „ ì„¸ê³„ì ìœ¼ë¡œ ì¸ê¸°ë¥¼ ëŒê³  ìˆìŠµë‹ˆë‹¤. BTSì™€ ë¸”ë™í•‘í¬ê°€ ë¹Œë³´ë“œ ì°¨íŠ¸ë¥¼ ì„ê¶Œí–ˆìŠµë‹ˆë‹¤.",
    "ë„·í”Œë¦­ìŠ¤ì™€ ë””ì¦ˆë‹ˆí”ŒëŸ¬ìŠ¤ê°€ í•œêµ­ ì½˜í…ì¸ ì— ëŒ€ê·œëª¨ íˆ¬ìë¥¼ í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
    "ë©”íƒ€ë²„ìŠ¤ ì‹œì¥ì´ ìƒˆë¡œìš´ ê¸°íšŒë¥¼ ì°½ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ë§ì€ ê¸°ì—…ë“¤ì´ ë©”íƒ€ë²„ìŠ¤ í”Œë«í¼ì„ ê°œë°œ ì¤‘ì…ë‹ˆë‹¤."
]

# ìœ ì‚¬ë„ ì¸¡ì •ìš© ì¿¼ë¦¬
QUERY_SENTENCES = [
    "ì£¼ì‹ì‹œì¥ ì „ë§ì€ ì–´ë–¤ê°€ìš”?",
    "AI ê¸°ìˆ ì´ ìš°ë¦¬ ìƒí™œì— ë¯¸ì¹˜ëŠ” ì˜í–¥",
    "ë¶€ë™ì‚° íˆ¬ì ì‹œê¸°",
    "ë°˜ë„ì²´ ì‚°ì—…ì˜ ë¯¸ë˜",
    "ê¸ˆë¦¬ ì¸ìƒì˜ ì˜í–¥"
]


class EmbeddingBenchmark:
    """ì„ë² ë”© ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ í´ë˜ìŠ¤"""

    def __init__(self):
        self.results = {}
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"Device: {self.device}")
        if self.device == "cuda":
            print(f"GPU: {torch.cuda.get_device_name(0)}")
            print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

    def benchmark_model(self, model_name: str, model_id: str) -> Dict[str, Any]:
        """ë‹¨ì¼ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬"""
        print(f"\n{'='*60}")
        print(f"Testing: {model_name} ({model_id})")
        print(f"{'='*60}")

        result = {
            "model_name": model_name,
            "model_id": model_id,
            "device": self.device
        }

        try:
            # ëª¨ë¸ ë¡œë”© ì‹œê°„ ì¸¡ì •
            start_time = time.time()
            model = SentenceTransformer(model_id, device=self.device)
            load_time = time.time() - start_time
            result["load_time"] = load_time
            print(f"âœ… Model loaded in {load_time:.2f} seconds")

            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (GPUì¸ ê²½ìš°)
            if self.device == "cuda":
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                memory_used = torch.cuda.memory_allocated() / 1024**3
                result["gpu_memory_gb"] = memory_used
                print(f"ğŸ“Š GPU Memory: {memory_used:.2f} GB")

            # ì„ë² ë”© ìƒì„± ì†ë„ ì¸¡ì •
            print(f"\nğŸ”„ Encoding {len(TEST_SENTENCES_KO)} sentences...")
            start_time = time.time()
            embeddings = model.encode(TEST_SENTENCES_KO, show_progress_bar=False)
            encode_time = time.time() - start_time
            result["encode_time"] = encode_time
            result["sentences_per_second"] = len(TEST_SENTENCES_KO) / encode_time
            print(f"âš¡ Encoding speed: {result['sentences_per_second']:.1f} sentences/sec")

            # ì„ë² ë”© ì°¨ì›
            result["dimension"] = embeddings.shape[1]
            print(f"ğŸ“ Embedding dimension: {result['dimension']}")

            # ì¿¼ë¦¬-ë¬¸ì„œ ìœ ì‚¬ë„ ì¸¡ì •
            print(f"\nğŸ” Computing similarities...")
            query_embeddings = model.encode(QUERY_SENTENCES, show_progress_bar=False)

            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = np.dot(query_embeddings, embeddings.T)
            norms = np.linalg.norm(query_embeddings, axis=1)[:, np.newaxis] * np.linalg.norm(embeddings, axis=1)
            similarities = similarities / norms

            # í‰ê·  ìµœê³  ìœ ì‚¬ë„ (ê²€ìƒ‰ í’ˆì§ˆ ì§€í‘œ)
            max_similarities = similarities.max(axis=1)
            result["avg_max_similarity"] = float(max_similarities.mean())
            print(f"ğŸ“ˆ Average max similarity: {result['avg_max_similarity']:.3f}")

            # Top-3 ì •í™•ë„ (ê° ì¿¼ë¦¬ì— ëŒ€í•´ ìƒìœ„ 3ê°œ ë¬¸ì„œê°€ ê´€ë ¨ìˆëŠ”ì§€)
            top3_indices = similarities.argsort(axis=1)[:, -3:]
            result["top3_indices"] = top3_indices.tolist()

            # ë°°ì¹˜ ì²˜ë¦¬ ì†ë„ ì¸¡ì •
            batch_sizes = [1, 8, 32]
            batch_times = {}
            for batch_size in batch_sizes:
                start_time = time.time()
                for i in range(0, len(TEST_SENTENCES_KO), batch_size):
                    batch = TEST_SENTENCES_KO[i:i+batch_size]
                    _ = model.encode(batch, show_progress_bar=False)
                batch_time = time.time() - start_time
                batch_times[f"batch_{batch_size}"] = batch_time
            result["batch_times"] = batch_times
            print(f"ğŸ”„ Batch processing times: {batch_times}")

            # ì„±ê³µ
            result["status"] = "success"

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del model
            if self.device == "cuda":
                torch.cuda.empty_cache()

        except Exception as e:
            print(f"âŒ Error: {e}")
            result["status"] = "failed"
            result["error"] = str(e)

        return result

    def run_benchmarks(self):
        """ëª¨ë“  ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        models = [
            ("BGE-M3", "BAAI/bge-m3"),
            ("Multilingual-E5-Large", "intfloat/multilingual-e5-large"),
            ("Multilingual-E5-Base", "intfloat/multilingual-e5-base"),
            ("Ko-SRoBERTa", "jhgan/ko-sroberta-multitask"),
            ("KoSimCSE", "BM-K/KoSimCSE-roberta"),
            ("MiniLM", "sentence-transformers/all-MiniLM-L6-v2"),
        ]

        print("\nğŸš€ Starting Embedding Model Benchmarks")
        print(f"Testing {len(models)} models with {len(TEST_SENTENCES_KO)} Korean sentences")

        for model_name, model_id in models:
            result = self.benchmark_model(model_name, model_id)
            self.results[model_name] = result
            time.sleep(2)  # ëª¨ë¸ ê°„ ê°„ê²©

        self.print_summary()
        self.save_results()

    def print_summary(self):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½"""
        print("\n" + "="*80)
        print("ğŸ“Š BENCHMARK SUMMARY")
        print("="*80)

        # ì„±ê³µí•œ ëª¨ë¸ë§Œ í•„í„°ë§
        successful_models = {k: v for k, v in self.results.items() if v.get("status") == "success"}

        if not successful_models:
            print("âŒ No successful model tests")
            return

        # í…Œì´ë¸” í—¤ë”
        print(f"\n{'Model':<25} {'Dim':<6} {'Load(s)':<10} {'Speed(s/s)':<12} {'Similarity':<10} {'Memory(GB)':<10}")
        print("-" * 80)

        # ê° ëª¨ë¸ ê²°ê³¼ ì¶œë ¥
        for model_name, result in successful_models.items():
            dim = result.get('dimension', 0)
            load_time = result.get('load_time', 0)
            speed = result.get('sentences_per_second', 0)
            similarity = result.get('avg_max_similarity', 0)
            memory = result.get('gpu_memory_gb', 0)

            print(f"{model_name:<25} {dim:<6} {load_time:<10.2f} {speed:<12.1f} {similarity:<10.3f} {memory:<10.2f}")

        # ìµœì  ëª¨ë¸ ì¶”ì²œ
        print("\n" + "="*80)
        print("ğŸ† RECOMMENDATIONS")
        print("="*80)

        # ì†ë„ ìµœì 
        fastest = max(successful_models.items(), key=lambda x: x[1].get('sentences_per_second', 0))
        print(f"âš¡ Fastest: {fastest[0]} ({fastest[1]['sentences_per_second']:.1f} sentences/sec)")

        # í’ˆì§ˆ ìµœì 
        best_quality = max(successful_models.items(), key=lambda x: x[1].get('avg_max_similarity', 0))
        print(f"ğŸ¯ Best Quality: {best_quality[0]} (similarity: {best_quality[1]['avg_max_similarity']:.3f})")

        # ê· í˜• ì¶”ì²œ (ì†ë„ì™€ í’ˆì§ˆ ì ìˆ˜ ì¡°í•©)
        def balance_score(item):
            result = item[1]
            speed_norm = result.get('sentences_per_second', 0) / 100  # ì •ê·œí™”
            quality_norm = result.get('avg_max_similarity', 0)
            return speed_norm * 0.3 + quality_norm * 0.7  # í’ˆì§ˆì— ë” ê°€ì¤‘ì¹˜

        best_balanced = max(successful_models.items(), key=balance_score)
        print(f"âš–ï¸  Best Balanced: {best_balanced[0]}")

        print("\nğŸ’¡ Recommendation for Korean YouTube Content:")
        print("   1. BGE-M3: Best overall performance for multilingual content")
        print("   2. Ko-SRoBERTa: Best for Korean-only content")
        print("   3. Multilingual-E5-Base: Good balance of speed and quality")

    def save_results(self):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        output_file = "embedding_benchmark_results.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ“ Results saved to {output_file}")


if __name__ == "__main__":
    # í™˜ê²½ ì²´í¬
    print("ğŸ”§ Environment Check")
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")

    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark = EmbeddingBenchmark()

    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ (ë¹ ë¥¸ ëª¨ë¸ë§Œ)
    quick_test = True

    if quick_test:
        print("\nâš¡ Running quick test with selected models...")
        # ì£¼ìš” ëª¨ë¸ë§Œ í…ŒìŠ¤íŠ¸
        models = [
            ("BGE-M3", "BAAI/bge-m3"),
            ("Multilingual-E5-Base", "intfloat/multilingual-e5-base"),
            ("Ko-SRoBERTa", "jhgan/ko-sroberta-multitask"),
        ]

        for model_name, model_id in models:
            result = benchmark.benchmark_model(model_name, model_id)
            benchmark.results[model_name] = result

        benchmark.print_summary()
        benchmark.save_results()
    else:
        # ì „ì²´ ë²¤ì¹˜ë§ˆí¬
        benchmark.run_benchmarks()