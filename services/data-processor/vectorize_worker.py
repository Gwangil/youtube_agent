#!/usr/bin/env python3
"""
ë²¡í„°í™” ì „ìš© ì›Œì»¤
STT ì™„ë£Œëœ ì½˜í…ì¸ ë¥¼ ì¦‰ì‹œ ë²¡í„°í™”í•˜ì—¬ ì§€ì‹ë² ì´ìŠ¤ì— ì €ì¥
"""

import os
import sys
import time
import hashlib
from datetime import datetime
from typing import List, Dict
from sqlalchemy.orm import sessionmaker
from sqlalchemy import create_engine
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct
import re
import redis
import json
import pickle
from shared.utils.embeddings import get_embeddings

# Add project root to path
sys.path.append('/app')
sys.path.append('/app/shared')

from shared.models.database import (
    Content, Transcript, ProcessingJob, VectorMapping,
    get_database_url
)
from shared.utils.retry import retry, robust_retry


class VectorizeWorker:
    """ë²¡í„°í™” ì „ìš© ì›Œì»¤"""

    def __init__(self):
        self.worker_id = int(os.getenv('VECTORIZE_WORKER_ID', '0'))
        self.engine = create_engine(get_database_url())
        self.SessionLocal = sessionmaker(bind=self.engine)

        # Qdrant ì—°ê²°
        qdrant_url = os.getenv('QDRANT_URL', 'http://localhost:6333')
        self.qdrant_client = QdrantClient(url=qdrant_url)

        # í•˜ì´ë¸Œë¦¬ë“œ ì„ë² ë”© ì´ˆê¸°í™” (GPU/CPU ìë™ ì„ íƒ)
        self.embeddings = get_embeddings()
        model_info = self.embeddings.get_model_info()

        # Redis ìºì‹œ ì—°ê²°
        redis_url = os.getenv('REDIS_URL', 'redis://localhost:6379')
        self.redis_client = redis.from_url(redis_url, decode_responses=False)
        self.cache_ttl = 3600 * 24 * 7  # 7ì¼ê°„ ìºì‹œ ìœ ì§€

        print(f"ğŸš€ ë²¡í„°í™” ì „ìš© ì›Œì»¤ #{self.worker_id} ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"  ğŸ¤– ì„ë² ë”© ëª¨ë¸: {model_info['model_name']} ({model_info['model_type']})")
        print(f"  ğŸ’¾ ë””ë°”ì´ìŠ¤: {model_info['device']}")
        print(f"  ğŸ“¦ Redis ìºì‹œ ì—°ê²°ë¨")

    def get_db(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ìƒì„±"""
        return self.SessionLocal()

    def _create_semantic_chunks(self, transcripts: List[Transcript]) -> List[Dict]:
        """ë¬¸ì¥ ê¸°ë°˜ ì˜ë¯¸ ì²­í‚¹ (ê°•í™”ëœ ì¤‘ë³µ ì œê±°)"""
        chunks = []
        current_chunk = {
            'text': '',
            'start_time': 0,
            'end_time': 0,
            'sentences': []
        }

        # í…ìŠ¤íŠ¸ ì¤‘ë³µ ê°ì§€ë¥¼ ìœ„í•œ í•´ì‹œ ì„¸íŠ¸
        seen_texts = set()

        for transcript in transcripts:
            text = transcript.text.strip()
            if not text or len(text) < 5:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                continue

            # ì¤‘ë³µ í…ìŠ¤íŠ¸ í™•ì¸
            text_hash = hashlib.md5(text.lower().encode()).hexdigest()
            if text_hash in seen_texts:
                continue
            seen_texts.add(text_hash)

            # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•  (í•œêµ­ì–´ ë¬¸ì¥ ë íŒ¨í„´)
            sentences = re.split(r'[.!?ã€‚]+', text)
            sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 3]

            for sentence in sentences:
                # ë°˜ë³µ íŒ¨í„´ ì œê±°
                cleaned_sentence = self._clean_repetitive_text(sentence)
                if not cleaned_sentence or len(cleaned_sentence) < 5:
                    continue

                # ì²« ë²ˆì§¸ ë¬¸ì¥ì´ë©´ ì‹œì‘ ì‹œê°„ ì„¤ì •
                if not current_chunk['sentences']:
                    current_chunk['start_time'] = transcript.start_time

                current_chunk['sentences'].append(cleaned_sentence)
                current_chunk['text'] += cleaned_sentence + '. '
                current_chunk['end_time'] = transcript.end_time

                # ì²­í¬ í¬ê¸° ì œí•œ (1-3 ë¬¸ì¥ ë˜ëŠ” 200-600ìë¡œ ì¡°ì •)
                chunk_length = len(current_chunk['text'])
                sentence_count = len(current_chunk['sentences'])

                if sentence_count >= 2 or chunk_length >= 600:
                    # ì˜ë¯¸ ìˆëŠ” ì²­í¬ë§Œ ì¶”ê°€
                    if chunk_length > 10 and sentence_count > 0:
                        chunks.append(current_chunk.copy())
                    current_chunk = {
                        'text': '',
                        'start_time': 0,
                        'end_time': 0,
                        'sentences': []
                    }

        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk['sentences'] and len(current_chunk['text']) > 10:
            chunks.append(current_chunk)

        print(f"  ğŸ“Š ì²­í‚¹ í†µê³„: {len(transcripts)}ê°œ íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ -> {len(chunks)}ê°œ ì²­í¬")
        return chunks

    def _clean_repetitive_text(self, text: str) -> str:
        """ë°˜ë³µë˜ëŠ” í…ìŠ¤íŠ¸ íŒ¨í„´ ì œê±°"""
        if not text:
            return text

        # ì—°ì†ëœ ë™ì¼ ë‹¨ì–´ ì œê±°
        import re
        text = re.sub(r'\b(\w+)(\s+\1\b)+', r'\1', text)

        # ë™ì¼ êµ¬ë¬¸ ë°˜ë³µ ì œê±°
        words = text.split()
        if len(words) < 4:
            return text

        cleaned_words = []
        i = 0
        while i < len(words):
            max_pattern_length = min(3, (len(words) - i) // 2)
            pattern_found = False

            for pattern_len in range(max_pattern_length, 1, -1):
                if i + pattern_len * 2 <= len(words):
                    pattern = words[i:i+pattern_len]
                    next_pattern = words[i+pattern_len:i+pattern_len*2]

                    if pattern == next_pattern:
                        cleaned_words.extend(pattern)
                        i += pattern_len * 2
                        pattern_found = True
                        break

            if not pattern_found:
                cleaned_words.append(words[i])
                i += 1

        return ' '.join(cleaned_words)

    def _create_timestamp_url(self, original_url: str, start_time_seconds: float) -> str:
        """YouTube íƒ€ì„ìŠ¤íƒ¬í”„ URL ìƒì„±"""
        try:
            # URLì—ì„œ íŒŒë¼ë¯¸í„° ë¶„ë¦¬
            if '?' in original_url:
                base_url, params = original_url.split('?', 1)
                # ê¸°ì¡´ t íŒŒë¼ë¯¸í„° ì œê±°
                param_parts = [p for p in params.split('&') if not p.startswith('t=')]
                if param_parts:
                    url_without_timestamp = f"{base_url}?{'&'.join(param_parts)}"
                else:
                    url_without_timestamp = base_url
            else:
                url_without_timestamp = original_url

            # ì‹œê°„ì„ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜
            timestamp_seconds = int(start_time_seconds)

            # URLì— íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
            separator = '&' if '?' in url_without_timestamp else '?'
            return f"{url_without_timestamp}{separator}t={timestamp_seconds}s"

        except Exception:
            return original_url

    def process_vectorization(self, job: ProcessingJob):
        """ë²¡í„°í™” ì²˜ë¦¬"""
        print(f"ğŸ”§ ë²¡í„°í™” ì‘ì—… ì²˜ë¦¬: Job {job.id}")

        db = self.get_db()
        try:
            # ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸
            job.status = 'processing'
            job.started_at = datetime.utcnow()
            db.commit()

            # ì½˜í…ì¸ ì™€ íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ì¡°íšŒ
            content = db.query(Content).filter(Content.id == job.content_id).first()
            if not content:
                raise Exception("ì½˜í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            transcripts = db.query(Transcript).filter(
                Transcript.content_id == content.id
            ).order_by(Transcript.segment_order).all()

            if not transcripts:
                raise Exception("íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")

            print(f"  ğŸ“ {len(transcripts)}ê°œ íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ ì¤‘...")

            # ë¬¸ì¥ ê¸°ë°˜ ì²­í‚¹ ìˆ˜í–‰
            semantic_chunks = self._create_semantic_chunks(transcripts)
            print(f"  ğŸ§© {len(semantic_chunks)}ê°œ ì˜ë¯¸ ì²­í¬ ìƒì„±")

            # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
            batch_size = 100  # í•œ ë²ˆì— ì²˜ë¦¬í•  ì„ë² ë”© ìˆ˜
            points = []

            # ì²­í¬ í…ìŠ¤íŠ¸ ë°°ì¹˜ë¡œ ìˆ˜ì§‘
            for batch_start in range(0, len(semantic_chunks), batch_size):
                batch_end = min(batch_start + batch_size, len(semantic_chunks))
                batch_chunks = semantic_chunks[batch_start:batch_end]

                # ë°°ì¹˜ í…ìŠ¤íŠ¸ ì¤€ë¹„
                batch_texts = [chunk['text'] for chunk in batch_chunks]

                # ìºì‹œëœ ì„ë² ë”© í™•ì¸ ë° ìƒˆë¡œìš´ ì„ë² ë”© ìƒì„±
                batch_embeddings = []
                texts_to_embed = []  # ìºì‹œë˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸
                text_indices = []  # ì›ë³¸ ë°°ì¹˜ì—ì„œì˜ ì¸ë±ìŠ¤

                for idx, text in enumerate(batch_texts):
                    # ìºì‹œ í‚¤ ìƒì„± (í…ìŠ¤íŠ¸ í•´ì‹œ)
                    cache_key = f"embedding:{hashlib.md5(text.encode()).hexdigest()}"

                    # Redisì—ì„œ ìºì‹œëœ ì„ë² ë”© í™•ì¸
                    cached_embedding = self.redis_client.get(cache_key)

                    if cached_embedding:
                        # ìºì‹œ íˆíŠ¸
                        embedding = pickle.loads(cached_embedding)
                        batch_embeddings.append(embedding)
                    else:
                        # ìºì‹œ ë¯¸ìŠ¤ - ë‚˜ì¤‘ì— ì„ë² ë”©í•  í…ìŠ¤íŠ¸ë¡œ ì¶”ê°€
                        texts_to_embed.append(text)
                        text_indices.append(idx)
                        batch_embeddings.append(None)  # ìë¦¬í‘œì‹œì

                # ìºì‹œë˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸ë“¤ì— ëŒ€í•œ ë°°ì¹˜ ì„ë² ë”© ìƒì„±
                if texts_to_embed:
                    print(f"  ğŸ”„ ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì¤‘... (ì‹ ê·œ: {len(texts_to_embed)}ê°œ, ìºì‹œ: {len(batch_texts) - len(texts_to_embed)}ê°œ)")

                    # ì„ë² ë”© ìƒì„± (ì¬ì‹œë„ í¬í•¨)
                    @robust_retry()
                    def generate_embeddings():
                        return self.embeddings.embed_documents(texts_to_embed)

                    new_embeddings = generate_embeddings()

                    # ìƒì„±ëœ ì„ë² ë”©ì„ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ë°°ì¹˜í•˜ê³  ìºì‹œì— ì €ì¥
                    for text, embedding, original_idx in zip(texts_to_embed, new_embeddings, text_indices):
                        batch_embeddings[original_idx] = embedding

                        # Redisì— ìºì‹œ ì €ì¥
                        cache_key = f"embedding:{hashlib.md5(text.encode()).hexdigest()}"
                        self.redis_client.setex(
                            cache_key,
                            self.cache_ttl,
                            pickle.dumps(embedding)
                        )
                else:
                    print(f"  âœ… ëª¨ë“  ì„ë² ë”©ì´ ìºì‹œì—ì„œ ë¡œë“œë¨ ({len(batch_texts)}ê°œ)")

                # ê° ì²­í¬ì— ëŒ€í•œ í¬ì¸íŠ¸ ìƒì„±
                for idx, (chunk_data, embedding) in enumerate(zip(batch_chunks, batch_embeddings)):
                    i = batch_start + idx

                    # ì²­í¬ ID ìƒì„±
                    chunk_id = hashlib.md5(
                        f"{content.id}_{i}_{chunk_data['text'][:50]}".encode()
                    ).hexdigest()

                    # íƒ€ì„ìŠ¤íƒ¬í”„ URL ìƒì„±
                    timestamp_url = self._create_timestamp_url(content.url, chunk_data['start_time'])

                    # Qdrant í¬ì¸íŠ¸ ìƒì„±
                    point = PointStruct(
                        id=chunk_id,
                        vector=embedding,
                        payload={
                            "content_id": content.id,
                            "chunk_index": i,
                            "text": chunk_data['text'],
                            "start_time": chunk_data['start_time'],
                            "end_time": chunk_data['end_time'],
                            "title": content.title,
                            "channel_name": content.channel.name if content.channel else "Unknown",
                            "publish_date": content.publish_date.isoformat() if content.publish_date else None,
                            "url": content.url,
                            "timestamp_url": timestamp_url,
                            "language": content.language,
                            "duration": content.duration,
                            "transcript_type": content.transcript_type
                        }
                    )
                    points.append(point)

            # Qdrantì— ë²¡í„° ë°ì´í„° ì €ì¥ (ì¬ì‹œë„ í¬í•¨)
            @retry(max_attempts=3, delay=1.0)
            def upsert_to_qdrant():
                self.qdrant_client.upsert(
                    collection_name="youtube_content",
                    points=points
                )

            upsert_to_qdrant()

            # ë²¡í„° ë§¤í•‘ ì •ë³´ ì €ì¥
            for i, point in enumerate(points):
                vector_mapping = VectorMapping(
                    content_id=content.id,
                    chunk_index=i,
                    vector_id=point.id,
                    collection_name="youtube_content",
                    start_time=semantic_chunks[i]['start_time'],
                    end_time=semantic_chunks[i]['end_time'],
                    text_content=semantic_chunks[i]['text']
                )
                db.add(vector_mapping)

            # ì‘ì—… ì™„ë£Œ
            job.status = 'completed'
            job.completed_at = datetime.utcnow()
            db.commit()

            print(f"  âœ… ë²¡í„°í™” ì™„ë£Œ: {content.title[:50]}... ({len(points)}ê°œ ë²¡í„°)")

        except Exception as e:
            print(f"  âŒ ë²¡í„°í™” ì‹¤íŒ¨: {e}")
            job.status = 'failed'
            job.error_message = str(e)
            job.completed_at = datetime.utcnow()
            db.commit()
        finally:
            db.close()

    def start_worker(self):
        """ì›Œì»¤ ì‹œì‘ - ì›Œì»¤ ID ê¸°ë°˜ íŒŒí‹°ì…”ë‹ìœ¼ë¡œ ì‘ì—… ë¶„ì‚°"""
        print(f"ğŸš€ ë²¡í„°í™” ì „ìš© ì›Œì»¤ #{self.worker_id} ì‹œì‘")
        total_workers = int(os.getenv('TOTAL_VECTORIZE_WORKERS', '3'))
        print(f"  ì´ ì›Œì»¤ ìˆ˜: {total_workers}, ë‚´ ID: {self.worker_id}")

        while True:
            try:
                db = self.get_db()

                # ëª¨ë“  ëŒ€ê¸° ì¤‘ì¸ ë²¡í„°í™” ì‘ì—… ì¡°íšŒ
                all_jobs = db.query(ProcessingJob).filter(
                    ProcessingJob.job_type == 'vectorize',
                    ProcessingJob.status == 'pending'
                ).order_by(
                    ProcessingJob.priority.desc(),
                    ProcessingJob.created_at
                ).all()

                # ì´ ì›Œì»¤ê°€ ì²˜ë¦¬í•  ì‘ì—…ë§Œ í•„í„°ë§ (job.id % total_workers == worker_id)
                my_jobs = [job for job in all_jobs if job.id % total_workers == self.worker_id]

                if my_jobs:
                    # ìµœëŒ€ 5ê°œ ì‘ì—…ë§Œ ì²˜ë¦¬
                    for job in my_jobs[:5]:
                        print(f"\\nğŸ¯ [Worker {self.worker_id}] ë²¡í„°í™” ì‘ì—… ì„ íƒ: Job {job.id} (Priority: {job.priority})")
                        self.process_vectorization(job)
                        time.sleep(2)  # ì‘ì—… ê°„ ì§§ì€ ëŒ€ê¸°
                else:
                    print(f"ğŸ“­ [Worker {self.worker_id}] ëŒ€ê¸° ì¤‘ì¸ ë²¡í„°í™” ì‘ì—… ì—†ìŒ")

                db.close()
                time.sleep(10)  # 10ì´ˆë§ˆë‹¤ í™•ì¸

            except KeyboardInterrupt:
                print(f"ğŸ›‘ ë²¡í„°í™” ì›Œì»¤ #{self.worker_id} ì¢…ë£Œ")
                break
            except Exception as e:
                print(f"âŒ [Worker {self.worker_id}] ì›Œì»¤ ì˜¤ë¥˜: {e}")
                time.sleep(30)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    worker = VectorizeWorker()
    worker.start_worker()


if __name__ == "__main__":
    main()