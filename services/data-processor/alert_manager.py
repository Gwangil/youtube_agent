#!/usr/bin/env python3
"""
ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì‹œìŠ¤í…œ
ë¬¸ì œ ê°ì§€ ì‹œ ì•Œë¦¼ì„ ìƒì„±í•˜ê³  ê´€ë¦¬
"""

import os
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker
from qdrant_client import QdrantClient
import redis
import json
from enum import Enum

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AlertLevel(Enum):
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"

class AlertType(Enum):
    DATA_INTEGRITY = "data_integrity"
    PROCESSING_FAILURE = "processing_failure"
    SYSTEM_PERFORMANCE = "system_performance"
    RESOURCE_USAGE = "resource_usage"
    STUCK_JOBS = "stuck_jobs"
    DUPLICATE_DATA = "duplicate_data"

class AlertManager:
    def __init__(self):
        self.db_url = os.getenv("DATABASE_URL", "postgresql://youtube_user:youtube_pass@postgres:5432/youtube_agent")
        self.engine = create_engine(self.db_url)
        self.SessionLocal = sessionmaker(bind=self.engine)

        self.qdrant = QdrantClient(
            host=os.getenv("QDRANT_HOST", "qdrant"),
            port=int(os.getenv("QDRANT_PORT", 6333))
        )

        self.redis_client = redis.Redis(
            host=os.getenv("REDIS_HOST", "redis"),
            port=int(os.getenv("REDIS_PORT", 6379)),
            decode_responses=True
        )

        # ì•Œë¦¼ ì„ê³„ê°’
        self.thresholds = {
            'stuck_jobs': 10,  # 10ê°œ ì´ìƒ ë©ˆì¶˜ ì‘ì—…
            'failed_jobs': 20,  # 20ê°œ ì´ìƒ ì‹¤íŒ¨í•œ ì‘ì—…
            'processing_lag': 100,  # 100ê°œ ì´ìƒ ëŒ€ê¸° ì¤‘ì¸ ì‘ì—…
            'integrity_issues': 5,  # 5ê°œ ì´ìƒ ì •í•©ì„± ë¬¸ì œ
            'duplicate_rate': 0.1,  # 10% ì´ìƒ ì¤‘ë³µë¥ 
        }

    def check_and_alert(self) -> List[Dict]:
        """ì „ì²´ ì‹œìŠ¤í…œ ì²´í¬ ë° ì•Œë¦¼ ìƒì„±"""
        logger.info(f"ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì‹œì‘: {datetime.now()}")

        alerts = []

        # 1. ë°ì´í„° ì •í•©ì„± ì²´í¬
        integrity_alerts = self._check_data_integrity()
        alerts.extend(integrity_alerts)

        # 2. ì²˜ë¦¬ ì‘ì—… ìƒíƒœ ì²´í¬
        job_alerts = self._check_processing_jobs()
        alerts.extend(job_alerts)

        # 3. ì‹œìŠ¤í…œ ì„±ëŠ¥ ì²´í¬
        performance_alerts = self._check_system_performance()
        alerts.extend(performance_alerts)

        # 4. ì¤‘ë³µ ë°ì´í„° ì²´í¬
        duplicate_alerts = self._check_duplicate_data()
        alerts.extend(duplicate_alerts)

        # ì•Œë¦¼ ì €ì¥ ë° ì „ì†¡
        if alerts:
            self._save_alerts(alerts)
            self._send_notifications(alerts)

        logger.info(f"ëª¨ë‹ˆí„°ë§ ì™„ë£Œ: {len(alerts)}ê°œ ì•Œë¦¼ ìƒì„±")

        return alerts

    def _check_data_integrity(self) -> List[Dict]:
        """ë°ì´í„° ì •í•©ì„± ì²´í¬"""
        alerts = []
        session = self.SessionLocal()

        try:
            # transcript í”Œë˜ê·¸ ë¶ˆì¼ì¹˜
            query = text("""
                SELECT COUNT(*) FROM content c
                WHERE (
                    (transcript_available = TRUE AND NOT EXISTS (
                        SELECT 1 FROM transcripts t WHERE t.content_id = c.id
                    ))
                    OR
                    (transcript_available = FALSE AND EXISTS (
                        SELECT 1 FROM transcripts t WHERE t.content_id = c.id
                    ))
                )
            """)
            result = session.execute(query).scalar()

            if result > self.thresholds['integrity_issues']:
                alerts.append({
                    'type': AlertType.DATA_INTEGRITY.value,
                    'level': AlertLevel.WARNING.value,
                    'title': 'Transcript í”Œë˜ê·¸ ë¶ˆì¼ì¹˜',
                    'message': f'{result}ê°œ ì½˜í…ì¸ ì˜ transcript í”Œë˜ê·¸ê°€ ì‹¤ì œ ë°ì´í„°ì™€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.',
                    'count': result,
                    'timestamp': datetime.now().isoformat()
                })

            # vector í”Œë˜ê·¸ ë¶ˆì¼ì¹˜
            query = text("""
                SELECT COUNT(*) FROM content c
                WHERE (
                    (vector_stored = TRUE AND NOT EXISTS (
                        SELECT 1 FROM vector_mappings v WHERE v.content_id = c.id
                    ))
                    OR
                    (vector_stored = FALSE AND EXISTS (
                        SELECT 1 FROM vector_mappings v WHERE v.content_id = c.id
                    ))
                )
            """)
            result = session.execute(query).scalar()

            if result > self.thresholds['integrity_issues']:
                alerts.append({
                    'type': AlertType.DATA_INTEGRITY.value,
                    'level': AlertLevel.WARNING.value,
                    'title': 'Vector í”Œë˜ê·¸ ë¶ˆì¼ì¹˜',
                    'message': f'{result}ê°œ ì½˜í…ì¸ ì˜ vector í”Œë˜ê·¸ê°€ ì‹¤ì œ ë°ì´í„°ì™€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.',
                    'count': result,
                    'timestamp': datetime.now().isoformat()
                })

            # ê³ ì•„ ë°ì´í„°
            query = text("""
                SELECT
                    (SELECT COUNT(*) FROM transcripts t
                     WHERE NOT EXISTS (SELECT 1 FROM content c WHERE c.id = t.content_id)) as orphan_transcripts,
                    (SELECT COUNT(*) FROM vector_mappings v
                     WHERE NOT EXISTS (SELECT 1 FROM content c WHERE c.id = v.content_id)) as orphan_vectors
            """)
            result = session.execute(query).first()

            if result[0] > 0 or result[1] > 0:
                alerts.append({
                    'type': AlertType.DATA_INTEGRITY.value,
                    'level': AlertLevel.ERROR.value,
                    'title': 'ê³ ì•„ ë°ì´í„° ë°œê²¬',
                    'message': f'ê³ ì•„ transcript: {result[0]}ê°œ, ê³ ì•„ vector: {result[1]}ê°œ',
                    'orphan_transcripts': result[0],
                    'orphan_vectors': result[1],
                    'timestamp': datetime.now().isoformat()
                })

        except Exception as e:
            logger.error(f"ì •í•©ì„± ì²´í¬ ì‹¤íŒ¨: {e}")
        finally:
            session.close()

        return alerts

    def _check_processing_jobs(self) -> List[Dict]:
        """ì²˜ë¦¬ ì‘ì—… ìƒíƒœ ì²´í¬"""
        alerts = []
        session = self.SessionLocal()

        try:
            # ë©ˆì¶˜ ì‘ì—…
            query = text("""
                SELECT COUNT(*) FROM processing_jobs
                WHERE status = 'processing'
                AND created_at < NOW() - INTERVAL '30 minutes'
            """)
            stuck_count = session.execute(query).scalar()

            if stuck_count > self.thresholds['stuck_jobs']:
                alerts.append({
                    'type': AlertType.STUCK_JOBS.value,
                    'level': AlertLevel.ERROR.value,
                    'title': 'ë©ˆì¶˜ ì‘ì—… ê³¼ë‹¤',
                    'message': f'{stuck_count}ê°œ ì‘ì—…ì´ 30ë¶„ ì´ìƒ processing ìƒíƒœì…ë‹ˆë‹¤.',
                    'count': stuck_count,
                    'timestamp': datetime.now().isoformat()
                })

            # ì‹¤íŒ¨í•œ ì‘ì—…
            query = text("""
                SELECT COUNT(*), job_type
                FROM processing_jobs
                WHERE status = 'failed'
                AND created_at > NOW() - INTERVAL '1 day'
                GROUP BY job_type
            """)
            failures = session.execute(query).fetchall()

            for count, job_type in failures:
                if count > self.thresholds['failed_jobs']:
                    alerts.append({
                        'type': AlertType.PROCESSING_FAILURE.value,
                        'level': AlertLevel.WARNING.value,
                        'title': f'{job_type} ì‘ì—… ì‹¤íŒ¨ ê³¼ë‹¤',
                        'message': f'ìµœê·¼ 24ì‹œê°„ ë™ì•ˆ {count}ê°œ {job_type} ì‘ì—…ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.',
                        'job_type': job_type,
                        'count': count,
                        'timestamp': datetime.now().isoformat()
                    })

            # ëŒ€ê¸° ì¤‘ì¸ ì‘ì—…
            query = text("""
                SELECT COUNT(*), job_type
                FROM processing_jobs
                WHERE status = 'pending'
                GROUP BY job_type
            """)
            pending = session.execute(query).fetchall()

            for count, job_type in pending:
                if count > self.thresholds['processing_lag']:
                    alerts.append({
                        'type': AlertType.PROCESSING_FAILURE.value,
                        'level': AlertLevel.WARNING.value,
                        'title': f'{job_type} ì‘ì—… ì§€ì—°',
                        'message': f'{count}ê°œ {job_type} ì‘ì—…ì´ ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤.',
                        'job_type': job_type,
                        'count': count,
                        'timestamp': datetime.now().isoformat()
                    })

        except Exception as e:
            logger.error(f"ì‘ì—… ì²´í¬ ì‹¤íŒ¨: {e}")
        finally:
            session.close()

        return alerts

    def _check_system_performance(self) -> List[Dict]:
        """ì‹œìŠ¤í…œ ì„±ëŠ¥ ì²´í¬"""
        alerts = []
        session = self.SessionLocal()

        try:
            # ì²˜ë¦¬ ì†ë„ ì²´í¬
            query = text("""
                SELECT
                    AVG(EXTRACT(EPOCH FROM (
                        CASE WHEN status = 'completed' THEN created_at ELSE NOW() END
                        - created_at
                    ))) as avg_processing_time,
                    job_type
                FROM processing_jobs
                WHERE created_at > NOW() - INTERVAL '1 hour'
                GROUP BY job_type
            """)
            results = session.execute(query).fetchall()

            for avg_time, job_type in results:
                if avg_time and avg_time > 1800:  # 30ë¶„ ì´ìƒ
                    alerts.append({
                        'type': AlertType.SYSTEM_PERFORMANCE.value,
                        'level': AlertLevel.WARNING.value,
                        'title': f'{job_type} ì²˜ë¦¬ ì†ë„ ì €í•˜',
                        'message': f'{job_type} í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_time/60:.1f}ë¶„',
                        'job_type': job_type,
                        'avg_time': avg_time,
                        'timestamp': datetime.now().isoformat()
                    })

            # ì—ëŸ¬ìœ¨ ì²´í¬
            query = text("""
                SELECT
                    job_type,
                    SUM(CASE WHEN status = 'failed' THEN 1 ELSE 0 END) as failed,
                    COUNT(*) as total
                FROM processing_jobs
                WHERE created_at > NOW() - INTERVAL '1 hour'
                GROUP BY job_type
            """)
            results = session.execute(query).fetchall()

            for job_type, failed, total in results:
                if total > 0:
                    error_rate = failed / total
                    if error_rate > 0.2:  # 20% ì´ìƒ ì—ëŸ¬ìœ¨
                        alerts.append({
                            'type': AlertType.SYSTEM_PERFORMANCE.value,
                            'level': AlertLevel.ERROR.value,
                            'title': f'{job_type} ì—ëŸ¬ìœ¨ ë†’ìŒ',
                            'message': f'{job_type} ì—ëŸ¬ìœ¨: {error_rate*100:.1f}% ({failed}/{total})',
                            'job_type': job_type,
                            'error_rate': error_rate,
                            'timestamp': datetime.now().isoformat()
                        })

        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ì²´í¬ ì‹¤íŒ¨: {e}")
        finally:
            session.close()

        return alerts

    def _check_duplicate_data(self) -> List[Dict]:
        """ì¤‘ë³µ ë°ì´í„° ì²´í¬"""
        alerts = []

        try:
            collections = ['youtube_content', 'youtube_summaries']

            for collection in collections:
                # ë²¡í„° DB í¬ì¸íŠ¸ ìˆ˜ í™•ì¸
                try:
                    collection_info = self.qdrant.get_collection(collection)
                    points_count = collection_info.points_count

                    # ì˜ˆìƒ í¬ì¸íŠ¸ ìˆ˜ì™€ ë¹„êµ
                    session = self.SessionLocal()
                    query = text("""
                        SELECT COUNT(DISTINCT content_id)
                        FROM vector_mappings
                        WHERE collection_name = :collection
                    """)
                    content_count = session.execute(
                        query,
                        {'collection': collection}
                    ).scalar() or 0
                    session.close()

                    if content_count > 0:
                        avg_chunks_per_content = points_count / content_count

                        # í‰ê·  ì²­í¬ ìˆ˜ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ë†’ì€ ê²½ìš°
                        if collection == 'youtube_content' and avg_chunks_per_content > 500:
                            alerts.append({
                                'type': AlertType.DUPLICATE_DATA.value,
                                'level': AlertLevel.WARNING.value,
                                'title': f'{collection} ì¤‘ë³µ ì˜ì‹¬',
                                'message': f'ì½˜í…ì¸ ë‹¹ í‰ê·  {avg_chunks_per_content:.1f}ê°œ ë²¡í„° (ì •ìƒ: 100-300ê°œ)',
                                'collection': collection,
                                'points_count': points_count,
                                'content_count': content_count,
                                'timestamp': datetime.now().isoformat()
                            })

                except Exception as e:
                    logger.error(f"{collection} ì²´í¬ ì‹¤íŒ¨: {e}")

        except Exception as e:
            logger.error(f"ì¤‘ë³µ ì²´í¬ ì‹¤íŒ¨: {e}")

        return alerts

    def _save_alerts(self, alerts: List[Dict]):
        """ì•Œë¦¼ ì €ì¥"""
        try:
            # Redisì— ì €ì¥
            for alert in alerts:
                # ê°œë³„ ì•Œë¦¼ ì €ì¥
                alert_id = f"alert:{datetime.now().strftime('%Y%m%d%H%M%S')}:{alert['type']}"
                self.redis_client.setex(alert_id, 86400, json.dumps(alert))

                # íƒ€ì…ë³„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                list_key = f"alerts:{alert['type']}"
                self.redis_client.lpush(list_key, json.dumps(alert))
                self.redis_client.ltrim(list_key, 0, 99)  # ìµœê·¼ 100ê°œë§Œ ìœ ì§€

            # ìµœì‹  ì•Œë¦¼ ì—…ë°ì´íŠ¸
            if alerts:
                self.redis_client.setex(
                    "alerts:latest",
                    3600,
                    json.dumps(alerts)
                )

        except Exception as e:
            logger.error(f"ì•Œë¦¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _send_notifications(self, alerts: List[Dict]):
        """ì•Œë¦¼ ì „ì†¡"""
        for alert in alerts:
            # ë¡œê·¸ë¡œ ì¶œë ¥
            level = alert.get('level', 'info')
            if level == AlertLevel.CRITICAL.value:
                logger.critical(f"ğŸš¨ {alert['title']}: {alert['message']}")
            elif level == AlertLevel.ERROR.value:
                logger.error(f"âŒ {alert['title']}: {alert['message']}")
            elif level == AlertLevel.WARNING.value:
                logger.warning(f"âš ï¸ {alert['title']}: {alert['message']}")
            else:
                logger.info(f"â„¹ï¸ {alert['title']}: {alert['message']}")

            # ì—¬ê¸°ì— ì´ë©”ì¼, Slack ë“± ì‹¤ì œ ì•Œë¦¼ ì „ì†¡ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥

    def get_recent_alerts(self, limit: int = 10) -> List[Dict]:
        """ìµœê·¼ ì•Œë¦¼ ì¡°íšŒ"""
        try:
            alerts_json = self.redis_client.get("alerts:latest")
            if alerts_json:
                alerts = json.loads(alerts_json)
                return alerts[:limit]
        except Exception as e:
            logger.error(f"ì•Œë¦¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        return []

def run_monitoring_service(interval_seconds: int = 300):
    """ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤ ì‹¤í–‰"""
    manager = AlertManager()

    logger.info(f"ëª¨ë‹ˆí„°ë§ ì•Œë¦¼ ì„œë¹„ìŠ¤ ì‹œì‘ (ê°„ê²©: {interval_seconds}ì´ˆ)")

    while True:
        try:
            # ì²´í¬ ë° ì•Œë¦¼
            alerts = manager.check_and_alert()

            # ëŒ€ê¸°
            time.sleep(interval_seconds)

        except KeyboardInterrupt:
            logger.info("ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤ ì¢…ë£Œ")
            break
        except Exception as e:
            logger.error(f"ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {e}")
            time.sleep(30)

if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "once":
        # í•œ ë²ˆë§Œ ì‹¤í–‰
        manager = AlertManager()
        alerts = manager.check_and_alert()
        print(json.dumps(alerts, indent=2, ensure_ascii=False))
    else:
        # ì„œë¹„ìŠ¤ ëª¨ë“œ
        run_monitoring_service()