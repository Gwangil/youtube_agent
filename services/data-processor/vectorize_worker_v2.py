#!/usr/bin/env python3
"""
í–¥ìƒëœ ë²¡í„°í™” ì›Œì»¤ (íŠ¸ëœì­ì…˜ ê´€ë¦¬ ì ìš©)
- ì›ìì  ì²˜ë¦¬ ë³´ì¥
- ì‹¤íŒ¨ ì‹œ ìë™ ë¡¤ë°±
- ë°ì´í„° ì •í•©ì„± ìœ ì§€
"""

import sys
import os
import time
import logging
from typing import List, Dict
import json

# ê²½ë¡œ ì„¤ì •
sys.path.append('/app')
sys.path.append('/app/services/data-integrity')

from shared.models.database import get_database_url, ProcessingJob, Content, Transcript
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker
from qdrant_client import QdrantClient
import redis
from transaction_manager import TransactionManager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class VectorizeWorkerV2:
    def __init__(self, worker_id: int = 0):
        self.worker_id = worker_id

        # DB ì—°ê²°
        self.engine = create_engine(get_database_url())
        self.SessionLocal = sessionmaker(bind=self.engine)

        # Qdrant ì—°ê²°
        self.qdrant = QdrantClient(
            host=os.getenv("QDRANT_HOST", "qdrant"),
            port=int(os.getenv("QDRANT_PORT", 6333))
        )

        # Redis ì—°ê²°
        self.redis = redis.Redis(
            host=os.getenv("REDIS_HOST", "redis"),
            port=int(os.getenv("REDIS_PORT", 6379)),
            decode_responses=True
        )

        logger.info(f"âœ… í–¥ìƒëœ ë²¡í„°í™” ì›Œì»¤ #{worker_id} ì´ˆê¸°í™” ì™„ë£Œ")

    def process_job(self, job: ProcessingJob):
        """ì‘ì—… ì²˜ë¦¬ (íŠ¸ëœì­ì…˜ ê´€ë¦¬ ì ìš©)"""
        content_id = job.content_id

        with self.SessionLocal() as db:
            # íŠ¸ëœì­ì…˜ ê´€ë¦¬ì ìƒì„±
            tx_manager = TransactionManager(db, self.qdrant, self.redis)

            try:
                # ì›ìì  ì‘ì—… ì‹œì‘
                with tx_manager.atomic_operation(content_id, "vectorization"):
                    logger.info(f"ğŸ”„ ë²¡í„°í™” ì‹œì‘: Content {content_id}")

                    # 1. ì½˜í…ì¸ ì™€ íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ì¡°íšŒ
                    content = db.query(Content).filter(Content.id == content_id).first()
                    if not content:
                        raise ValueError(f"ì½˜í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {content_id}")

                    # ë¹„í™œì„± ì½˜í…ì¸ ëŠ” ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
                    if hasattr(content, 'is_active') and not content.is_active:
                        logger.info(f"â­ï¸ ì½˜í…ì¸  {content_id}ê°€ ë¹„í™œì„±í™”ë¨, ì‘ì—… ì·¨ì†Œ")
                        job.status = 'cancelled'
                        job.error_message = 'Content is inactive'
                        db.commit()
                        return

                    transcripts = db.query(Transcript).filter(
                        Transcript.content_id == content_id
                    ).order_by(Transcript.segment_order).all()

                    if not transcripts:
                        raise ValueError(f"íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ì—†ìŒ: Content {content_id}")

                    # 2. ì²­í‚¹ ìƒì„±
                    chunks = self._create_chunks(transcripts, content)
                    logger.info(f"ğŸ“„ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

                    # 3. ì„ë² ë”© ìƒì„±
                    embeddings = self._generate_embeddings([c["text"] for c in chunks])

                    # 4. ê¸°ì¡´ ë²¡í„° ì œê±° (íŠ¸ëœì­ì…˜ ë‚´)
                    tx_manager.remove_vectors(content_id)

                    # 5. ìƒˆ ë²¡í„° ì¶”ê°€ (íŠ¸ëœì­ì…˜ ë‚´)
                    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
                        vector_data = {
                            "chunk_id": f"{content_id}_{i}",
                            "text": chunk["text"],
                            "vector": embedding,
                            "order": i,
                            "metadata": {
                                "title": content.title,
                                "url": content.url,
                                "start_time": chunk.get("start_time"),
                                "end_time": chunk.get("end_time"),
                                "timestamp_url": chunk.get("timestamp_url")
                            }
                        }

                        # content ì»¬ë ‰ì…˜ì— ì¶”ê°€
                        tx_manager.add_vector(
                            content_id,
                            "youtube_content",
                            vector_data
                        )

                    # 6. ìš”ì•½ ìƒì„± ë° ì €ì¥
                    summary = self._generate_summary(transcripts, content)
                    summary_embedding = self._generate_embeddings([summary["text"]])[0]

                    summary_data = {
                        "chunk_id": f"{content_id}_summary",
                        "text": summary["text"],
                        "vector": summary_embedding,
                        "order": 0,
                        "metadata": {
                            "title": content.title,
                            "url": content.url,
                            "type": "summary"
                        }
                    }

                    # summaries ì»¬ë ‰ì…˜ì— ì¶”ê°€
                    tx_manager.add_vector(
                        content_id,
                        "youtube_summaries",
                        summary_data
                    )

                    # 7. ì‘ì—… ì™„ë£Œ ì²˜ë¦¬
                    job.status = "completed"
                    job.updated_at = time.time()
                    db.commit()

                    logger.info(f"âœ… ë²¡í„°í™” ì™„ë£Œ: Content {content_id} ({len(chunks)} chunks + 1 summary)")

                    # 8. ìºì‹œ ë¬´íš¨í™”
                    self.redis.delete(f"cache:content:{content_id}:*")

            except Exception as e:
                # íŠ¸ëœì­ì…˜ ë§¤ë‹ˆì €ê°€ ìë™ìœ¼ë¡œ ë¡¤ë°± ì²˜ë¦¬
                logger.error(f"âŒ ë²¡í„°í™” ì‹¤íŒ¨ (Content {content_id}): {e}")

                # ì‘ì—… ì‹¤íŒ¨ ì²˜ë¦¬
                job.status = "failed"
                job.error_message = str(e)
                job.retry_count += 1
                job.updated_at = time.time()
                db.commit()

                raise

    def _create_chunks(self, transcripts: List[Transcript], content: Content) -> List[Dict]:
        """ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹"""
        chunks = []
        current_chunk = {
            "text": "",
            "start_time": None,
            "end_time": None
        }

        for transcript in transcripts:
            # ì²­í¬ í¬ê¸° ì²´í¬ (300-800ì)
            if len(current_chunk["text"]) + len(transcript.text) > 800:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                if current_chunk["text"]:
                    # íƒ€ì„ìŠ¤íƒ¬í”„ URL ìƒì„±
                    if current_chunk["start_time"] is not None:
                        current_chunk["timestamp_url"] = self._create_timestamp_url(
                            content.url,
                            current_chunk["start_time"]
                        )
                    chunks.append(current_chunk)

                # ìƒˆ ì²­í¬ ì‹œì‘
                current_chunk = {
                    "text": transcript.text,
                    "start_time": transcript.start_time,
                    "end_time": transcript.end_time
                }
            else:
                # ê¸°ì¡´ ì²­í¬ì— ì¶”ê°€
                if current_chunk["text"]:
                    current_chunk["text"] += " " + transcript.text
                else:
                    current_chunk["text"] = transcript.text
                    current_chunk["start_time"] = transcript.start_time

                current_chunk["end_time"] = transcript.end_time

        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk["text"]:
            if current_chunk["start_time"] is not None:
                current_chunk["timestamp_url"] = self._create_timestamp_url(
                    content.url,
                    current_chunk["start_time"]
                )
            chunks.append(current_chunk)

        return chunks

    def _create_timestamp_url(self, url: str, start_time: float) -> str:
        """YouTube íƒ€ì„ìŠ¤íƒ¬í”„ URL ìƒì„±"""
        if not url or start_time is None:
            return url

        timestamp_seconds = int(start_time)

        if "youtube.com" in url or "youtu.be" in url:
            # ê¸°ì¡´ íƒ€ì„ìŠ¤íƒ¬í”„ ì œê±°
            base_url = url.split("&t=")[0].split("?t=")[0]

            # ìƒˆ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
            separator = "&" if "?" in base_url else "?"
            return f"{base_url}{separator}t={timestamp_seconds}s"

        return url

    def _generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """ì„ë² ë”© ìƒì„± (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì„ë² ë”© ì„œë²„ í˜¸ì¶œ)"""
        # TODO: ì‹¤ì œ ì„ë² ë”© ì„œë²„ í˜¸ì¶œ
        import random
        return [[random.random() for _ in range(1024)] for _ in texts]

    def _generate_summary(self, transcripts: List[Transcript], content: Content) -> Dict:
        """ìš”ì•½ ìƒì„±"""
        full_text = " ".join([t.text for t in transcripts])

        # ê°„ë‹¨í•œ ìš”ì•½ (ì‹¤ì œë¡œëŠ” LLM ì‚¬ìš©)
        summary_text = f"{content.title}. " + full_text[:500]

        return {
            "text": summary_text
        }

    def run(self):
        """ì›Œì»¤ ì‹¤í–‰"""
        logger.info(f"ğŸš€ í–¥ìƒëœ ë²¡í„°í™” ì›Œì»¤ #{self.worker_id} ì‹œì‘")

        while True:
            with self.SessionLocal() as db:
                try:
                    # ëŒ€ê¸° ì¤‘ì¸ ë²¡í„°í™” ì‘ì—… ì¡°íšŒ
                    job = db.query(ProcessingJob).filter(
                        ProcessingJob.job_type == "vectorize",
                        ProcessingJob.status == "pending"
                    ).order_by(
                        ProcessingJob.priority.desc(),
                        ProcessingJob.created_at
                    ).first()

                    if job:
                        logger.info(f"ğŸ“‹ ì‘ì—… ì‹œì‘: Job {job.id} (Content {job.content_id})")

                        # ì‘ì—… ìƒíƒœë¥¼ processingìœ¼ë¡œ ë³€ê²½
                        job.status = "processing"
                        job.updated_at = time.time()
                        db.commit()

                        # ì‘ì—… ì²˜ë¦¬
                        self.process_job(job)

                    else:
                        # ì‘ì—…ì´ ì—†ìœ¼ë©´ ëŒ€ê¸°
                        time.sleep(10)

                except Exception as e:
                    logger.error(f"ì›Œì»¤ ì˜¤ë¥˜: {e}")
                    time.sleep(5)

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="í–¥ìƒëœ ë²¡í„°í™” ì›Œì»¤")
    parser.add_argument("--worker-id", type=int, default=0, help="ì›Œì»¤ ID")

    args = parser.parse_args()

    worker = VectorizeWorkerV2(args.worker_id)
    worker.run()