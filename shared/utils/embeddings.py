"""
í•˜ì´ë¸Œë¦¬ë“œ ì„ë² ë”© ê´€ë¦¬ì
GPU/CPU í™˜ê²½ì— ë”°ë¼ ë¡œì»¬ ë˜ëŠ” OpenAI API ìë™ ì„ íƒ
"""

import os
import torch
import logging
from typing import List, Optional
from langchain_openai import OpenAIEmbeddings

logger = logging.getLogger(__name__)


class HybridEmbeddings:
    """
    ì¸í”„ë¼ ê¸°ë°˜ ìë™ ì„ë² ë”© ëª¨ë¸ ì„ íƒ
    GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ ë¡œì»¬ ëª¨ë¸, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ OpenAI API
    """

    def __init__(self, prefer_local: bool = True, model_cache_dir: str = "/app/models"):
        self.prefer_local = prefer_local
        self.model_cache_dir = model_cache_dir
        self.device_info = self._get_device_info()
        self.embedding_model = None
        self.model_type = None

        # ëª¨ë¸ ì´ˆê¸°í™”
        self._initialize_model()

    def _get_device_info(self) -> dict:
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ì¡°íšŒ"""
        info = {
            "has_cuda": torch.cuda.is_available(),
            "cuda_device_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
            "cpu_count": os.cpu_count(),
            "device": "cuda" if torch.cuda.is_available() else "cpu"
        }

        if info["has_cuda"]:
            info["gpu_name"] = torch.cuda.get_device_name(0)
            info["gpu_memory_gb"] = torch.cuda.get_device_properties(0).total_memory / 1024**3

        return info

    def _initialize_model(self):
        """í™˜ê²½ì— ë”°ë¥¸ ìµœì  ëª¨ë¸ ì„ íƒ ë° ì´ˆê¸°í™”"""

        # 1. GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ BGE-M3 ëª¨ë¸ ìš°ì„  ì‚¬ìš©
        if self.device_info["has_cuda"]:
            try:
                self._init_bge_m3_gpu()
                logger.info("ğŸš€ BGE-M3 GPU ëª¨ë¸ ì‚¬ìš© - ë¡œì»¬ ì²˜ë¦¬ë¡œ ë¹ ë¥´ê³  ì•ˆì „í•©ë‹ˆë‹¤")
                return
            except Exception as e:
                logger.warning(f"GPU BGE-M3 ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨, OpenAI APIë¡œ í´ë°±: {e}")

        # 2. BGE-M3 ì‹¤íŒ¨ ì‹œ OpenAI API ì‚¬ìš© (í´ë°±)
        try:
            self._init_openai_model()
            logger.info("ğŸ“¡ OpenAI API ì‚¬ìš© ì¤‘ (BGE-M3 ì‚¬ìš© ë¶ˆê°€)")
            return
        except Exception as e:
            logger.warning(f"OpenAI API ì´ˆê¸°í™”ë„ ì‹¤íŒ¨: {e}")

        # 3. ìµœí›„ì˜ ìˆ˜ë‹¨ìœ¼ë¡œ CPU ë¡œì»¬ ëª¨ë¸ (ê¶Œì¥í•˜ì§€ ì•ŠìŒ)
        if self.device_info["cpu_count"] >= 4:
            try:
                self._init_local_model_cpu()
                logger.warning("âš ï¸ CPU ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© ì¤‘ - ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
                return
            except Exception as e:
                logger.error(f"CPU ë¡œì»¬ ëª¨ë¸ ì´ˆê¸°í™”ë„ ì‹¤íŒ¨: {e}")

        raise Exception("ì„ë² ë”© ëª¨ë¸ì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    def _init_bge_m3_gpu(self):
        """BGE-M3 ëª¨ë¸ ì „ìš© GPU ì´ˆê¸°í™” (1024ì°¨ì›)"""
        try:
            from sentence_transformers import SentenceTransformer

            model_name = "BAAI/bge-m3"
            logger.info(f"BGE-M3 ëª¨ë¸ ë¡œë”© ì¤‘ (1024ì°¨ì›)...")

            self.embedding_model = SentenceTransformer(
                model_name,
                device='cuda',
                cache_folder=self.model_cache_dir
            )
            self.model_type = "bge_m3_gpu"
            self.model_name = model_name
            self.dimension = 1024  # BGE-M3ëŠ” 1024ì°¨ì›
            logger.info(f"âœ… BGE-M3 GPU ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ (1024ì°¨ì›)")

        except ImportError:
            raise Exception("sentence-transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        except Exception as e:
            raise Exception(f"BGE-M3 ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")

    def _init_local_model_gpu(self):
        """GPUìš© ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            from sentence_transformers import SentenceTransformer

            # GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ëª¨ë¸ ì„ íƒ
            if self.device_info.get("gpu_memory_gb", 0) >= 3:
                # BGE-M3: ìµœê³  ì„±ëŠ¥ ë‹¤êµ­ì–´ ëª¨ë¸
                model_name = "BAAI/bge-m3"
            elif self.device_info.get("gpu_memory_gb", 0) >= 2:
                # í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸
                model_name = "jhgan/ko-sroberta-multitask"
            else:
                # ê²½ëŸ‰ ë‹¤êµ­ì–´ ëª¨ë¸
                model_name = "intfloat/multilingual-e5-base"

            logger.info(f"GPU ë¡œì»¬ ëª¨ë¸ ë¡œë”©: {model_name}")
            self.embedding_model = SentenceTransformer(
                model_name,
                device='cuda',
                cache_folder=self.model_cache_dir
            )
            self.model_type = "local_gpu"
            self.model_name = model_name
            logger.info(f"âœ… GPU ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ: {model_name}")

        except ImportError:
            raise Exception("sentence-transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    def _init_local_model_cpu(self):
        """CPUìš© ê²½ëŸ‰ ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            from sentence_transformers import SentenceTransformer

            # CPUìš© ê²½ëŸ‰ í•œêµ­ì–´ ëª¨ë¸
            # CPU ì½”ì–´ ìˆ˜ì— ë”°ë¼ ëª¨ë¸ ì„ íƒ
            if self.device_info["cpu_count"] >= 8:
                model_name = "jhgan/ko-sroberta-multitask"  # í•œêµ­ì–´ íŠ¹í™”
            else:
                model_name = "intfloat/multilingual-e5-base"  # ê²½ëŸ‰ ë‹¤êµ­ì–´

            logger.info(f"CPU ë¡œì»¬ ëª¨ë¸ ë¡œë”©: {model_name}")
            self.embedding_model = SentenceTransformer(
                model_name,
                device='cpu',
                cache_folder=self.model_cache_dir
            )
            self.model_type = "local_cpu"
            self.model_name = model_name
            logger.info(f"âœ… CPU ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ: {model_name}")

        except ImportError:
            raise Exception("sentence-transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    def _init_openai_model(self):
        """OpenAI API ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (1024ì°¨ì›ìœ¼ë¡œ í†µì¼)"""
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        # text-embedding-3-largeë¥¼ 1024ì°¨ì›ìœ¼ë¡œ ì„¤ì • (BGE-M3ì™€ ë™ì¼)
        self.embedding_model = OpenAIEmbeddings(
            openai_api_key=api_key,
            model="text-embedding-3-large",
            dimensions=1024  # BGE-M3ì™€ ë™ì¼í•œ ì°¨ì› ì„¤ì •
        )
        self.model_type = "openai_api"
        self.model_name = "text-embedding-3-large-1024"
        self.dimension = 1024  # ëª…ì‹œì ìœ¼ë¡œ ì°¨ì› ì„¤ì •
        logger.info("âœ… OpenAI API ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ (1024ì°¨ì›)")

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ì—¬ëŸ¬ ë¬¸ì„œ ì„ë² ë”© (ë°°ì¹˜ ì²˜ë¦¬)"""
        if self.model_type in ["bge_m3_gpu", "local_gpu", "local_cpu"]:
            # ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©
            embeddings = self.embedding_model.encode(
                texts,
                batch_size=32 if "gpu" in self.model_type else 8,
                show_progress_bar=False,
                convert_to_numpy=True
            )
            return embeddings.tolist()
        else:
            # OpenAI API ì‚¬ìš©
            return self.embedding_model.embed_documents(texts)

    def embed_query(self, text: str) -> List[float]:
        """ë‹¨ì¼ ì¿¼ë¦¬ ì„ë² ë”©"""
        if self.model_type in ["bge_m3_gpu", "local_gpu", "local_cpu"]:
            # ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©
            embedding = self.embedding_model.encode(
                text,
                show_progress_bar=False,
                convert_to_numpy=True
            )
            return embedding.tolist()
        else:
            # OpenAI API ì‚¬ìš©
            return self.embedding_model.embed_query(text)

    def get_model_info(self) -> dict:
        """í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "model_type": self.model_type,
            "model_name": self.model_name,
            "dimension": getattr(self, 'dimension', 1024),  # ê¸°ë³¸ 1024ì°¨ì›
            "device": self.device_info["device"],
            "device_info": self.device_info
        }

    def benchmark(self, test_texts: List[str] = None) -> dict:
        """ì„ë² ë”© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        import time

        if test_texts is None:
            test_texts = [
                "ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤.",
                "The quick brown fox jumps over the lazy dog.",
                "í•œêµ­ì–´ì™€ ì˜ì–´ë¥¼ ëª¨ë‘ í¬í•¨í•œ ë¬¸ì¥ì…ë‹ˆë‹¤."
            ] * 10

        start_time = time.time()
        embeddings = self.embed_documents(test_texts)
        elapsed_time = time.time() - start_time

        return {
            "model_type": self.model_type,
            "model_name": self.model_name,
            "num_texts": len(test_texts),
            "total_time": elapsed_time,
            "avg_time_per_text": elapsed_time / len(test_texts),
            "embedding_dim": len(embeddings[0]) if embeddings else 0
        }


# ì „ì—­ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_hybrid_embeddings = None


def get_embeddings(force_reload: bool = False) -> HybridEmbeddings:
    """ì‹±ê¸€í†¤ ì„ë² ë”© ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _hybrid_embeddings

    if _hybrid_embeddings is None or force_reload:
        _hybrid_embeddings = HybridEmbeddings()

    return _hybrid_embeddings